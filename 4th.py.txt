#!/usr/bin/env python
# coding: utf-8

# In[1]:


#  Import libariers
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd


# In[2]:


# import Mall_customers Dataset:

dataset =  pd.read_csv('Mall_Customers.csv')


# In[3]:


dataset.head()


# In[4]:


dataset.shape


# In[5]:


print(dataset.columns)


# In[6]:


dataset.info()


# In[7]:


dataset.describe().T


# In[8]:


dataset.isnull().sum()


# In[ ]:





# In[9]:


# Extracting Independent Variables
# Here we don't need any dependent variable for data pre-processing step as it is a clustering problem, and
# we have no idea about what to determine. 
# get the 'Annual Income (k$)', 'Spending Score (1-100) featuers


x = dataset.iloc[:, [3, 4]] 


# In[10]:


x.head()


# In[11]:


x.size


# In[12]:


plt.scatter(dataset['Annual Income (k$)'],dataset['Spending Score (1-100)'])

Step-2: Finding the optimal number of clusters using the elbow method

In the second step, we will try to find the optimal number of clusters for our clustering problem.
we use the elbow method to find optimal number of clusters.

elbow method uses the WCSS concept to draw the plot by plotting WCSS values on the Y-axis and the number of clusters on the X-axis. 

So we are going to calculate the value for WCSS for different k values ranging from 1 to 10. Below is the code for it:

# In[13]:


#finding optimal number of clusters using the elbow method  

from sklearn.cluster import KMeans  

wcss_list = []  #Initializing the list for the values of WCSS  

# Using for loop for iterations from 1 to 10.  
for i in range(1, 11):  
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state= 42)  
    kmeans.fit(x)  
    wcss_list.append(kmeans.inertia_)  

plt.plot(range(1, 11), wcss_list)  
plt.title('The Elobw Method Graph')  
plt.xlabel('Number of clusters(k)')  
plt.ylabel('wcss_list')  
plt.show()  

 


# In[14]:


len(wcss_list)


# # The model : 

# In[30]:


from sklearn.cluster import KMeans

1#training the K-means model on a dataset  

kmeans = KMeans(n_clusters = 5, init='k-means++', random_state= 42)  
y_predict= kmeans.fit_predict(x)  

#centroid
print(" Cluster centroids are \n", kmeans.cluster_centers_)

print(" \n\n predicated clusters for data points are :", y_predict)


# In[16]:


x


# In[17]:


y_predict


# In[ ]:




# Step-4: Visualizing the Clusters

As we have 5 clusters for our model, so we will visualize each cluster one by one.

To visualize the clusters will use scatter plot using scatter() function of matplotlib.

# In[32]:


# visulaizing the clusters  

plt.scatter(x[y_predict == 0, 0], x[y_predict == 0, 1], s = 100, c = 'blue', label = 'Cluster 1') #for first cluster  
plt.scatter(x[y_predict == 1, 0], x[y_predict == 1, 1], s = 100, c = 'green', label = 'Cluster 2') #for second cluster  
plt.scatter(x[y_predict== 2, 0], x[y_predict == 2, 1], s = 100, c = 'red', label = 'Cluster 3') #for third cluster  
plt.scatter(x[y_predict == 3, 0], x[y_predict == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4') #for fourth cluster  
plt.scatter(x[y_predict == 4, 0], x[y_predict == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5') #for fifth cluster  



plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroid')   
plt.title('Clusters of customers')  
plt.xlabel('Annual Income (k$)')  
plt.ylabel('Spending Score (1-100)')  
plt.legend()  
plt.show()  


# In[19]:


# display to which cluster customer belongs


dataset['cluster']=y_predict
dataset


# In[20]:


# Customers id in a cluster
# For example 
# Customers  in cluster 2 are

cust1=dataset[dataset["cluster"]==2]
print('Number of customer in 1st group=', len(cust1))
print('They are -', cust1["CustomerID"].values)
print("--------------------------------------------")


# In[ ]:





# In[21]:


x=dataset.iloc[:,[3,4]].values
#print(x)


# In[22]:


import scipy.cluster.hierarchy as shc

dendro = shc.dendrogram(shc.linkage(x, method="ward"))
plt.title("Dendrogram Plot")

plt.show()


# In[23]:


#  Agglomative clustering method


# In[ ]:




